{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1df90ca5",
   "metadata": {
    "id": "SI0CCRYGbsLX",
    "papermill": {
     "duration": 0.003558,
     "end_time": "2025-06-10T06:25:24.789550",
     "exception": false,
     "start_time": "2025-06-10T06:25:24.785992",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 第7回講義 宿題"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220f624d",
   "metadata": {
    "id": "jhZjl_2gbsLf",
    "papermill": {
     "duration": 0.002504,
     "end_time": "2025-06-10T06:25:24.795221",
     "exception": false,
     "start_time": "2025-06-10T06:25:24.792717",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 課題\n",
    "RNNを用いてIMDbのsentiment analysisを実装してみましょう．\n",
    "\n",
    "ネットワークの形などに制限はとくになく，今回のLessonで扱った内容以外の工夫も組み込んでもらって構いません．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "603faafe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T06:25:24.801862Z",
     "iopub.status.busy": "2025-06-10T06:25:24.801559Z",
     "iopub.status.idle": "2025-06-10T06:25:38.913853Z",
     "shell.execute_reply": "2025-06-10T06:25:38.912798Z"
    },
    "id": "4X9ptijybsLz",
    "outputId": "cfa8ac80-87e6-4d30-aa00-179949d9897c",
    "papermill": {
     "duration": 14.117333,
     "end_time": "2025-06-10T06:25:38.915218",
     "exception": false,
     "start_time": "2025-06-10T06:25:24.797885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting portalocker\r\n",
      "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\r\n",
      "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\r\n",
      "Installing collected packages: portalocker\r\n",
      "Successfully installed portalocker-3.1.1\r\n",
      "単語種数: 88587\n"
     ]
    }
   ],
   "source": [
    "!pip install portalocker\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "from typing import List, Union\n",
    "\n",
    "seed = 1234\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "# 学習データ\n",
    "x_train = np.load('/kaggle/input/imdb-dataset/x_train.npy', allow_pickle=True)\n",
    "t_train = np.load('/kaggle/input/imdb-dataset/t_train.npy', allow_pickle=True)\n",
    "\n",
    "# 検証データを取る\n",
    "x_train, x_valid, t_train, t_valid = train_test_split(x_train, t_train, test_size=0.2, random_state=seed)\n",
    "\n",
    "# テストデータ\n",
    "x_test = np.load('/kaggle/input/imdb-dataset/x_test.npy', allow_pickle=True)\n",
    "\n",
    "\n",
    "def text_transform(text: List[int], max_length=256):\n",
    "    # <BOS>はすでに1で入っている．<EOS>は2とする．\n",
    "    text = text[:max_length - 1] + [2]\n",
    "\n",
    "    return text, len(text)\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, len_seq_list = [], [], []\n",
    "\n",
    "    for sample in batch:\n",
    "        if isinstance(sample, tuple):\n",
    "            label, text = sample\n",
    "\n",
    "            label_list.append(label)\n",
    "        else:\n",
    "            text = sample.copy()\n",
    "\n",
    "        text, len_seq = text_transform(text)\n",
    "        text_list.append(torch.tensor(text))\n",
    "        len_seq_list.append(len_seq)\n",
    "\n",
    "    # NOTE: 宿題用データセットでは<PAD>は3です．\n",
    "    return torch.tensor(label_list), pad_sequence(text_list, padding_value=3).T, torch.tensor(len_seq_list)\n",
    "\n",
    "\n",
    "word_num = np.concatenate(np.concatenate((x_train, x_test))).max() + 1\n",
    "print(f\"単語種数: {word_num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c034147e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T06:25:38.922578Z",
     "iopub.status.busy": "2025-06-10T06:25:38.922236Z",
     "iopub.status.idle": "2025-06-10T06:25:38.934751Z",
     "shell.execute_reply": "2025-06-10T06:25:38.933934Z"
    },
    "id": "EIQ-km5gkVB8",
    "outputId": "af845edd-995a-4902-b240-b44eafe8018a",
    "papermill": {
     "duration": 0.017548,
     "end_time": "2025-06-10T06:25:38.936086",
     "exception": false,
     "start_time": "2025-06-10T06:25:38.918538",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "2494\n"
     ]
    }
   ],
   "source": [
    "min_len = min(len(x) for x in x_train)\n",
    "print(min_len) # Max 7 tokens\n",
    "max_len = max(len(x) for x in x_train)\n",
    "print(max_len) # Min 2494 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc8998d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T06:25:38.943167Z",
     "iopub.status.busy": "2025-06-10T06:25:38.942583Z",
     "iopub.status.idle": "2025-06-10T06:25:38.946225Z",
     "shell.execute_reply": "2025-06-10T06:25:38.945259Z"
    },
    "id": "2Y31ypAzt0bM",
    "papermill": {
     "duration": 0.008424,
     "end_time": "2025-06-10T06:25:38.947502",
     "exception": false,
     "start_time": "2025-06-10T06:25:38.939078",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (backbone = LSTM -> Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "376843c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T06:25:38.954188Z",
     "iopub.status.busy": "2025-06-10T06:25:38.953955Z",
     "iopub.status.idle": "2025-06-10T06:25:38.959750Z",
     "shell.execute_reply": "2025-06-10T06:25:38.959017Z"
    },
    "id": "cjA6ePA5esZ3",
    "outputId": "61135a21-ad8e-4b8c-8fd7-f1ed340e4009",
    "papermill": {
     "duration": 0.010496,
     "end_time": "2025-06-10T06:25:38.960893",
     "exception": false,
     "start_time": "2025-06-10T06:25:38.950397",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32000,), (32000,), (8000,), (8000,), (10000,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, t_train.shape, x_valid.shape, t_valid.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4723b8c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T06:25:38.967892Z",
     "iopub.status.busy": "2025-06-10T06:25:38.967326Z",
     "iopub.status.idle": "2025-06-10T06:25:38.971154Z",
     "shell.execute_reply": "2025-06-10T06:25:38.970405Z"
    },
    "id": "7PRRgV-pympm",
    "papermill": {
     "duration": 0.008379,
     "end_time": "2025-06-10T06:25:38.972306",
     "exception": false,
     "start_time": "2025-06-10T06:25:38.963927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed(seed=1234):\n",
    "  torch.manual_seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d47a86",
   "metadata": {
    "id": "WIKgr01wbsL5",
    "papermill": {
     "duration": 0.00268,
     "end_time": "2025-06-10T06:25:38.977796",
     "exception": false,
     "start_time": "2025-06-10T06:25:38.975116",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f505fb56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T06:25:38.984270Z",
     "iopub.status.busy": "2025-06-10T06:25:38.984059Z",
     "iopub.status.idle": "2025-06-10T06:25:39.012720Z",
     "shell.execute_reply": "2025-06-10T06:25:39.012163Z"
    },
    "id": "WPhmbQtagOvH",
    "papermill": {
     "duration": 0.033135,
     "end_time": "2025-06-10T06:25:39.013934",
     "exception": false,
     "start_time": "2025-06-10T06:25:38.980799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    [(t, x) for t, x in zip(t_train, x_train)],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch,\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    [(t, x) for t, x in zip(t_valid, x_valid)],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    x_test,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db0a9397",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T06:25:39.021493Z",
     "iopub.status.busy": "2025-06-10T06:25:39.021240Z",
     "iopub.status.idle": "2025-06-10T06:25:39.258530Z",
     "shell.execute_reply": "2025-06-10T06:25:39.257909Z"
    },
    "id": "9oMBBydx-FqL",
    "papermill": {
     "duration": 0.242736,
     "end_time": "2025-06-10T06:25:39.260072",
     "exception": false,
     "start_time": "2025-06-10T06:25:39.017336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MultiLayerBiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim,\n",
    "                 n_layers, dropout, padding_idx):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. 埋め込み層\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "\n",
    "        # 2. 多層双方向LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=n_layers,      # レイヤー数を指定\n",
    "            bidirectional=True,       # 双方向を有効化\n",
    "            batch_first=True,\n",
    "            dropout=dropout           # 層間のドロップアウトを指定\n",
    "        )\n",
    "\n",
    "        # 3. 分類器ヘッド (双方向なので hidden_dim * 2)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "        # 4. 追加のドロップアウト層\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text, text_lengths):\n",
    "        # text = [batch size, sent len]\n",
    "\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        # embedded = [batch size, sent len, emb dim]\n",
    "\n",
    "        # パディングを無視するためのpack処理 (オプションだが推奨)\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'), batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "\n",
    "        # hidden = [num_layers * num_directions, batch_size, hid_dim]\n",
    "        # 最後の層の隠れ状態を連結\n",
    "        last_hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        # last_hidden = [batch size, hid dim * 2]\n",
    "\n",
    "        return self.fc(last_hidden)\n",
    "\n",
    "# モデルのインスタンス化\n",
    "VOCAB_SIZE = 88587 # 例\n",
    "EMBEDDING_DIM = 200\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2  # 2層から試す\n",
    "DROPOUT = 0.4 # ドロップアウト率\n",
    "PADDING_IDX = 3\n",
    "\n",
    "model = MultiLayerBiLSTM(VOCAB_SIZE,\n",
    "                         EMBEDDING_DIM,\n",
    "                         HIDDEN_DIM,\n",
    "                         OUTPUT_DIM,\n",
    "                         N_LAYERS,\n",
    "                         DROPOUT,\n",
    "                         PADDING_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf42fe37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T06:25:39.267689Z",
     "iopub.status.busy": "2025-06-10T06:25:39.267445Z",
     "iopub.status.idle": "2025-06-10T06:32:21.084447Z",
     "shell.execute_reply": "2025-06-10T06:32:21.083622Z"
    },
    "id": "9w3yjqqCl_Zb",
    "outputId": "f6864b16-62d0-4626-8a88-72da8e18f2b3",
    "papermill": {
     "duration": 401.825619,
     "end_time": "2025-06-10T06:32:21.089006",
     "exception": false,
     "start_time": "2025-06-10T06:25:39.263387",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0, Train Loss: 0.641, Valid Loss: 0.535, Validation F1: 0.741\n",
      "EPOCH: 0, Train Loss: 0.641, Valid Loss: 0.535, Validation F1: 0.741\n",
      "✨ New best model saved at epoch 0 with F1: 0.7412\n",
      "EPOCH: 1, Train Loss: 0.555, Valid Loss: 0.653, Validation F1: 0.606\n",
      "EPOCH: 1, Train Loss: 0.555, Valid Loss: 0.653, Validation F1: 0.606\n",
      "EPOCH: 2, Train Loss: 0.448, Valid Loss: 0.449, Validation F1: 0.817\n",
      "EPOCH: 2, Train Loss: 0.448, Valid Loss: 0.449, Validation F1: 0.817\n",
      "✨ New best model saved at epoch 2 with F1: 0.8167\n",
      "EPOCH: 3, Train Loss: 0.349, Valid Loss: 0.348, Validation F1: 0.856\n",
      "EPOCH: 3, Train Loss: 0.349, Valid Loss: 0.348, Validation F1: 0.856\n",
      "✨ New best model saved at epoch 3 with F1: 0.8560\n",
      "EPOCH: 4, Train Loss: 0.299, Valid Loss: 0.305, Validation F1: 0.874\n",
      "EPOCH: 4, Train Loss: 0.299, Valid Loss: 0.305, Validation F1: 0.874\n",
      "✨ New best model saved at epoch 4 with F1: 0.8736\n",
      "EPOCH: 5, Train Loss: 0.257, Valid Loss: 0.352, Validation F1: 0.860\n",
      "EPOCH: 5, Train Loss: 0.257, Valid Loss: 0.352, Validation F1: 0.860\n",
      "EPOCH: 6, Train Loss: 0.228, Valid Loss: 0.309, Validation F1: 0.875\n",
      "EPOCH: 6, Train Loss: 0.228, Valid Loss: 0.309, Validation F1: 0.875\n",
      "✨ New best model saved at epoch 6 with F1: 0.8753\n",
      "EPOCH: 7, Train Loss: 0.201, Valid Loss: 0.296, Validation F1: 0.888\n",
      "EPOCH: 7, Train Loss: 0.201, Valid Loss: 0.296, Validation F1: 0.888\n",
      "✨ New best model saved at epoch 7 with F1: 0.8882\n",
      "EPOCH: 8, Train Loss: 0.174, Valid Loss: 0.292, Validation F1: 0.890\n",
      "EPOCH: 8, Train Loss: 0.174, Valid Loss: 0.292, Validation F1: 0.890\n",
      "✨ New best model saved at epoch 8 with F1: 0.8905\n",
      "EPOCH: 9, Train Loss: 0.150, Valid Loss: 0.348, Validation F1: 0.888\n",
      "EPOCH: 9, Train Loss: 0.150, Valid Loss: 0.348, Validation F1: 0.888\n",
      "\n",
      "Training finished. Best model was at epoch 8 with F1 score: 0.8905\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# 1. モデル定義 (修正版)\n",
    "# ===================================================================\n",
    "class MultiLayerBiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim,\n",
    "                 n_layers, dropout): # 修正: padding_idxを引数から削除\n",
    "        super().__init__()\n",
    "        \n",
    "        # ✅ 修正: nn.Embeddingから padding_idx の指定を削除\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # 多層双方向LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # 分類器ヘッド\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "        # ドロップアウト層\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text, text_lengths):\n",
    "        # (forwardメソッドの中身は変更なし)\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'), batch_first=True, enforce_sorted=False)\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        last_hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        return self.fc(last_hidden)\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# 2. ハイパーパラメータとセットアップ (修正版)\n",
    "# ===================================================================\n",
    "# --- ハイパーパラメータ ---\n",
    "emb_dim = 200\n",
    "hid_dim = 256\n",
    "n_layers = 2\n",
    "dropout = 0.4\n",
    "n_epochs = 10\n",
    "learning_rate = 1e-3\n",
    "# padding_idx = 3 # この行は不要\n",
    "\n",
    "# --- デバイス設定 ---\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# --- モデル、損失関数、オプティマイザの初期化 ---\n",
    "# ✅ 修正: 新しいモデル定義に合わせてpadding_idxを渡さない\n",
    "model = MultiLayerBiLSTM(\n",
    "    vocab_size=word_num,\n",
    "    embedding_dim=emb_dim,\n",
    "    hidden_dim=hid_dim,\n",
    "    output_dim=1,\n",
    "    n_layers=n_layers,\n",
    "    dropout=dropout\n",
    ")\n",
    "model.to(device) # この行でエラーが発生しなくなるはずです\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "best_valid_f1 = 0.0\n",
    "best_epoch = 0\n",
    "model_save_path = \"best_model.pt\"\n",
    "\n",
    "\n",
    "# ===================================================================\n",
    "# 3. 学習ループ\n",
    "# ===================================================================\n",
    "for epoch in range(n_epochs):\n",
    "    losses_train = []\n",
    "    losses_valid = []\n",
    "    # --- 訓練パート ---\n",
    "    model.train() # ✅ 修正: `net`ではなく`model`\n",
    "    for label, line, len_seq in train_dataloader:\n",
    "        \n",
    "        # 勾配をリセット\n",
    "        optimizer.zero_grad() # ✅ 修正: `net`ではなく`optimizer`\n",
    "\n",
    "        # データをデバイスに移動\n",
    "        t = label.to(device)\n",
    "        x = line.to(device)\n",
    "        # len_seqはpack_padded_sequenceでCPUにある必要があるため、ここでは移動しない\n",
    "\n",
    "        # 順伝播\n",
    "        # ✅ 修正: モデルのforward関数に合わせて引数を渡す\n",
    "        logits = model(x, len_seq)\n",
    "        \n",
    "        # 損失計算 (BCEWithLogitsLossは (出力, ラベル) の順で引数を取る)\n",
    "        # squeeze()で余分な次元を削除し、ラベルtの型をfloatに変換\n",
    "        loss = criterion(logits.squeeze(), t.float())\n",
    "        \n",
    "        # 逆伝播\n",
    "        loss.backward()\n",
    "\n",
    "        # 勾配クリッピング\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # ✅ 修正: `net`ではなく`model`\n",
    "\n",
    "        # ✅ 必須: optimizer.step() を呼び出し、重みを更新\n",
    "        optimizer.step()\n",
    "\n",
    "        losses_train.append(loss.item())\n",
    "\n",
    "    # --- 検証パート ---\n",
    "    t_valid = []\n",
    "    y_pred = []\n",
    "    model.eval() # ✅ 修正: `net`ではなく`model`\n",
    "    \n",
    "    # ✅ 改善: `with torch.no_grad()`で囲み、計算効率を向上させる\n",
    "    with torch.no_grad():\n",
    "        for label, line, len_seq in valid_dataloader:\n",
    "            t = label.to(device)\n",
    "            x = line.to(device)\n",
    "\n",
    "            # 順伝播\n",
    "            logits = model(x, len_seq) # ✅ 修正: `net`ではなく`model`\n",
    "\n",
    "            # 損失計算\n",
    "            loss = criterion(logits.squeeze(), t.float())\n",
    "            \n",
    "            # 予測ラベルを計算 (ロジットにSigmoidを適用し、0.5を閾値とする)\n",
    "            pred = (torch.sigmoid(logits) > 0.5).long().squeeze()\n",
    "\n",
    "            t_valid.extend(t.tolist())\n",
    "            y_pred.extend(pred.tolist())\n",
    "            losses_valid.append(loss.item())\n",
    "\n",
    "    # --- エポックごとの結果を表示 ---\n",
    "    print('EPOCH: {}, Train Loss: {:.3f}, Valid Loss: {:.3f}, Validation F1: {:.3f}'.format(\n",
    "        epoch,\n",
    "        np.mean(losses_train),\n",
    "        np.mean(losses_valid),\n",
    "        f1_score(t_valid, y_pred, average='macro')\n",
    "    ))\n",
    "    current_f1 = f1_score(t_valid, y_pred, average='macro')\n",
    "    \n",
    "    print('EPOCH: {}, Train Loss: {:.3f}, Valid Loss: {:.3f}, Validation F1: {:.3f}'.format(\n",
    "        epoch,\n",
    "        np.mean(losses_train),\n",
    "        np.mean(losses_valid),\n",
    "        current_f1\n",
    "    ))\n",
    "    \n",
    "    # ✅ アーリーストッピングのロジック\n",
    "    if current_f1 > best_valid_f1:\n",
    "        best_valid_f1 = current_f1\n",
    "        best_epoch = epoch\n",
    "        # 最良モデルのパラメータを保存\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"✨ New best model saved at epoch {epoch} with F1: {current_f1:.4f}\")\n",
    "\n",
    "# 学習後\n",
    "print(f\"\\nTraining finished. Best model was at epoch {best_epoch} with F1 score: {best_valid_f1:.4f}\")\n",
    "# 最良モデルをロードして評価に使用\n",
    "# model.load_state_dict(torch.load(model_save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e96add34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T06:32:21.097030Z",
     "iopub.status.busy": "2025-06-10T06:32:21.096412Z",
     "iopub.status.idle": "2025-06-10T06:32:25.121607Z",
     "shell.execute_reply": "2025-06-10T06:32:25.120760Z"
    },
    "id": "dQ8xtUcHj6IR",
    "papermill": {
     "duration": 4.030398,
     "end_time": "2025-06-10T06:32:25.122803",
     "exception": false,
     "start_time": "2025-06-10T06:32:21.092405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total predictions: 10000\n",
      "submission.csv has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# 1. 最良モデルの読み込みと評価モードへの切り替え\n",
    "# (アーリーストッピングで保存したモデルパスを指定)\n",
    "model_save_path = \"best_model.pt\"\n",
    "model.load_state_dict(torch.load(model_save_path))\n",
    "model.eval()\n",
    "\n",
    "# 2. 予測結果を保存するリストを初期化\n",
    "y_pred = []\n",
    "\n",
    "# 3. `torch.no_grad()`で勾配計算を無効化\n",
    "with torch.no_grad():\n",
    "    # ✅ 修正: test_dataloaderはラベル(t)を返さないため、アンパックを修正\n",
    "    # test_dataloaderは (line, len_seq) を返す\n",
    "    for _, line, len_seq in test_dataloader:\n",
    "        \n",
    "        # ✅ 修正: データをデバイスに移動\n",
    "        x = line.to(device)\n",
    "        # len_seq はCPU上にある必要があるため、GPUには移動しない\n",
    "\n",
    "        # 順伝播\n",
    "        logits = model(x, len_seq)\n",
    "        \n",
    "        # ✅ 修正: 未定義変数hではなく、モデルの出力logitsを使用する\n",
    "        # ロジットにSigmoidを適用して確率に変換\n",
    "        y = torch.sigmoid(logits)\n",
    "\n",
    "        # 確率を0.5を閾値として0か1の予測ラベルに変換\n",
    "        # .squeeze()はバッチサイズが1の場合にエラーを起こす可能性があるため、\n",
    "        # .reshape(-1)で常に1次元ベクトルにしてからリストに変換するのが安全\n",
    "        pred = y.round().reshape(-1)\n",
    "\n",
    "        # 予測結果をリストに追加\n",
    "        y_pred.extend(pred.cpu().tolist())\n",
    "\n",
    "# 4. 提出用ファイルの作成\n",
    "print(f\"Total predictions: {len(y_pred)}\")\n",
    "submission = pd.Series(y_pred, name='label', dtype=int)\n",
    "submission.to_csv('submission.csv', header=True, index_label='id')\n",
    "\n",
    "print(\"submission.csv has been created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00e3e03",
   "metadata": {
    "id": "W-MRCNboHZNd",
    "papermill": {
     "duration": 0.003286,
     "end_time": "2025-06-10T06:32:25.129741",
     "exception": false,
     "start_time": "2025-06-10T06:32:25.126455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bfe433",
   "metadata": {
    "papermill": {
     "duration": 0.003099,
     "end_time": "2025-06-10T06:32:25.136105",
     "exception": false,
     "start_time": "2025-06-10T06:32:25.133006",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7624043,
     "sourceId": 12109318,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 428.525309,
   "end_time": "2025-06-10T06:32:28.667353",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-10T06:25:20.142044",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
